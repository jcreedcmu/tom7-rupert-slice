(*   -*- electric-indent-local-mode: -1 -*- *)
let
  import "stdlib.bovex"
  import "layout.bovex"
  import "palatino.bovex"
  import "cite.bovex"
  import "ruperts.bib"

  import "polyhedra.bovex"
  import "solution-table.bovex"

  do POLYHEDRA : (string * string * string * polyhedron-class) list
  do solution-table : (string * int * int * int *
                       (* Solution ratio, clearance, types *)
                       (float * float * method list) option) list

  val doc-title = "Some upsetting things about shapes"
  val doc-subtitle = "There’s no subtitle either"

  do set-doc-info {(PDFInfo)
                   width = inch 8.5,
                   height = inch 11.0,
                   creator = "BoVeX",
                   producer = "Huh?",
                   title = doc-title,
                   author = "Tom 7",
                   subject = "Documents do not need a 'subject'"
                   (* leaving out date, so it is the creation time. *) }

  do SECTION-FONT-SIZE := point 15.0
  do SUBSECTION-FONT-SIZE := point 14.0
  do SUBSUBSECTION-FONT-SIZE := point 13.0

  fun monospace-block lay =
    monospace-block-ext { left-indent = 0.0,
                          width = inch 3.4,
                          line-spacing = point 0.05,
                          font-size = point 10.0,
                          attrs = {(Object) } } lay


  val tt = fixedersys

  val cbrt = tt[∛]

  fun body-font lay = span
    {(Span)
     font-face = font-family PALATINO,
     font-size = point 12.0,
     line-spacing = point 1.5 } lay

  fun sup lay =
    span {(Span) font-size = 8.0,
          (* Don't rephrase the text in these. *)
          no-rephrase = true } lay

  fun footnote s = color 0x999999FF [([b[footnote]]: [s])]

  val next-figure-id = ref 1
  fun figure-id () =
    let val s = int-to-string (!next-figure-id)
    in
      next-figure-id := !next-figure-id + 1;
      layout s
    end

  val ROLL-PITCH-YAW = figure-id()

  val questionmark-png = image-load "questionmark.png"

  fun string-pad-left (s, n, c) =
    if string-size s >= n
    then s
    else string-pad-left (c ^ s, n, c)

  fun render-float f =
    let
      (* for positive f, return g * 10^e such that 1.0 <= g < 10.0. *)
      fun exp-notation f =
        if f >=. 10.0
        then let val (g, e) = exp-notation (f /. 10.0)
             in (g, e + 1)
             end
        else if f <. 1.0
             then
               let val (g, e) = exp-notation (f *. 10.0)
               in (g, e - 1)
               end
             else (f, 0)

      fun render-finite-float f =
        if f <. 0.0
        then [-[render-finite-float (0.0 -. f)]]
        else
          let
            val ipart : int = trunc f
            val fpart : float = int-to-float ipart -. f
          in
            if ipart >= 1000000 andalso ipart <= 100000000
            then layout (int-to-string ipart)
            else
              let
                val (g, exp) = exp-notation f
                (* now render g as a fixed number of decimal
                   points. *)
                val n = round (g *. 1000000.0)
                val s = int-to-string n
                do string-size s != 7
                  andthen fail ("huh? " ^ s)

              in
                if exp <= 5 andalso exp >= 0
                then
                  (* For small exponents, just show the full number. *)
                  layout (string-prefix (s, exp + 1)) ^^ [.] ^^
                  layout (substr-rest (s, exp + 1))
                else if exp >= -5 andalso exp < 0
                     then
                       let
                         val exp = 0 - exp
                         val z = string-pad-left ("", exp, "0")
                         val s = z ^ s
                       in
                         layout (string-prefix (s, 1)) ^^ [.] ^^
                         layout (substr-rest (s, 1))
                       end
                     else
                       layout (string-prefix (s, 1)) ^^ [.] ^^
                       layout (substr-rest (s, 1)) ^^
                       [×10[sup (layout (int-to-string exp))]]

              end
          end
    in
      if float-is-nan f then [NaN]
      else if f ==. float-inf then [∞]
           else if f ==. float-inf then [-∞]
                else render-finite-float f
    end

  fun get-table-entry nickname =
    case list-find (fn (n : string, v, f, e, s) =>
                    string-eq (nickname, n))
         solution-table of
         NONE => fail ("Missing entry for " ^ nickname)
       | SOME (_, v, e, f, sol) => (v, e, f, sol)

  (* As constants since maybe these become stylized in the future *)
  val TIER-F = [F]
  val TIER-D = [D]
  val TIER-C = [C]
  val TIER-B = [B]
  val TIER-A = [A]
  val TIER-S = [S]
  val TIER-SS = [SS]

  object Scorecard of
    { about : layout,
      shape-tier : layout,
      shape-tier-notes: layout,
      rupert-tier : layout,
      rupert-tier-notes: layout,
      factoid : layout }

  fun polyhedron-info (c : string) =
    case list-find-partial (fn (cc, full, human, pc) =>
                            if string-eq (cc, c)
                            then SOME (full, human, pc)
                            else NONE) POLYHEDRA of
      NONE => fail ("bad polyhedron codename " ^ c)
    | SOME r => r

  fun polyhedra-without-method (m : method) =
    let
      val code-names = list-map-partial
        (fn (codename, _, _, _, SOME(_, _, l)) =>
         if list-exists (fn mm => method-eq (m, mm)) l
         then NONE
         else SOME codename
       | _ => NONE)
        solution-table

      val human-names =
        list-map (fn code =>
                  let val (_, h, _) = polyhedron-info code
                  in h
                  end) code-names
    in
      string-list-join ", " human-names
    end

  fun make-scorecard scorecards (nickname : string,
                                 identifier : string,
                                 human-name : string,
                                 pclass) : layout =
    let
      val (vertices, edges, faces, sol) = get-table-entry nickname

      val card : obj =
        case map-find (scorecards, nickname) of
          NONE => {(Scorecard) about = [Missing scorecard info!]}
        | SOME c => c

      val class-prefix =
        case pclass of
          PLATONIC => "platonic"
        | ARCHIMEDEAN => "archimedean"
        | CATALAN => "catalan"

      val IMG-DIM = inch 1.4

      val shape-img =
        image-inline { img = image-load (class-prefix ^ "-" ^
                                         identifier ^ ".png"),
                       height = IMG-DIM, dy = 0.0 }

      val residue-handle =
        case sol of
          NONE => questionmark-png
        | SOME _ => image-load (identifier ^ "-residue.png")

      val residue-img =
        image-inline { img = residue-handle,
                       height = IMG-DIM, dy = 0.0 }

      val nl = hfill () ^^ break ()

      val soltext =
        case sol of
          NONE => b[Unsolved!] ^^ nl
        | SOME (ratio : float, clearance : float, _) =>
            [[b[Ratio]]: [render-float ratio][nl]] ^^
            [[b[Clearance]]: [render-float clearance][nl]]

      (* Could add optional explanations. *)
      val shape-tier =
        case card of
          {(Scorecard) shape-tier, shape-tier-notes} =>
            [[b[Tier (shape)]]: [shape-tier] [it [“[shape-tier-notes]”]][nl]]
        | {(Scorecard) shape-tier} => [[b[Tier (shape)]]: [shape-tier][nl]]
        | _ => []

      (* Could add optional explanations. *)
      val rupert-tier =
        case card of
          {(Scorecard) rupert-tier, rupert-tier-notes} =>
            [[b[Tier (Rupert)]]: [rupert-tier] [it [“[rupert-tier-notes]”]][nl]]
        | {(Scorecard) rupert-tier} => [[b[Tier (Rupert)]]: [rupert-tier][nl]]
        | _ => []

      val about =
        case card of
          {(Scorecard) about} => about
        | _ => [Not much is known about the [b (layout human-name)].]


      val factoid =
        case card of
          {(Scorecard) factoid} => [[b[Fun fact]]: [factoid][nl]]
        | _ => []

      fun capitalize "" = ""
        | capitalize s =
        let
          val a = string-sub ("a", 0)
          val z = string-sub ("z", 0)
          val c = string-sub (s, 0)
          val cap =
            if c >= a andalso c <= z
            then c - 32
            else c
        in
          codepoint-to-string cap ^ substr-rest (s, 1)
        end

    in
      [[shape-img][residue-img][nl]
       [size (point 15.0) (b (layout (capitalize human-name)))][nl]
       [b[Class]]: [case pclass of
          PLATONIC => [Platonic]
        | ARCHIMEDEAN => [Archimedean]
        | CATALAN => [Catalan]][nl]
       [about][nl]
       [b[Vertices]]: [layout (int-to-string vertices)][nl]
       [b[Edges]]: [layout (int-to-string edges)][nl]
       [b[Faces]]: [layout (int-to-string faces)][nl]
       [shape-tier]
       [soltext]
       [rupert-tier]
       [factoid]
       ]
    end

  val scorecards =
    map-from-list string-compare
    (("tetra",
      {(Scorecard)
       shape-tier = TIER-B,
       shape-tier-notes = [Too sharp.],
       rupert-tier = TIER-A,
       rupert-tier-notes = [You thought the shape itself was sharp?
                            This looks designed to puncture tires.],
       factoid = [For any reasonable way of counting, this is
                  the smallest possible polyhedron!]
       }) ::

     ("cube", {(Scorecard)
       about = [],
       shape-tier = TIER-A,
       shape-tier-notes = [Ya basic.],
       factoid = [The most famous cube, “ice cube,” is actually
                  an oxymoron since ice water crystals are
                  hexagonal.],
               }) ::

     ("dode", {(Scorecard)
       about = [The regular [b[dodecahedron]] is five regular pentagons
                glued together in the only way it can be done. Its
                dual is the icosahedron.],
       shape-tier = TIER-SS,
       factoid = [In more than one of Bertrand Russell’s
                  nightmares,[cite russell1954nightmares][
                      cite russell1961theologian]
                  the universe is shaped like a dodecahedron.]
               }) ::

     ("icos", {(Scorecard)
       about = [The [b[icosahedron]] is more commonly known as the D20.
                Somehow they managed to fit [it[twenty]] equilateral
                triangles on this!],
       shape-tier = TIER-SS,
               }) ::

     ("octa", {(Scorecard)
       about = [The [b[octahedron]] is neither here nor there, but it does
                deserve XXX],
       shape-tier = TIER-C,
       factoid = [The Egyptian “Pyramids” are actually octahedra, with their
                  bottom halves buried beneath the sand for stability.]
               }) ::

     ("ttetra", {(Scorecard)
       shape-tier = TIER-A,
       shape-tier-notes = [Improved safety wrt tetrahedron.],

                 }) ::

     ("cocta", {(Scorecard)
       shape-tier = TIER-A,
                }) ::

     ("tcube", {(Scorecard)
       about = [The [b[truncated cube]] just cuts the corners off
                the cube, such that all the edges are the same length.],
       shape-tier = TIER-F,
       shape-tier-notes = [Terrible. A worse version of the cube.],
       rupert-tier = TIER-B,
       rupert-tier-notes = [Notably chunky residue, although it's basically
                            the same as the cube's.]
                }) ::

     ("tocta", {(Scorecard)
       about = [The [b[truncated octahedron]] improves upon the octahedron
                by replacing its corners with squares.],
       shape-tier = TIER-B,
       factoid = [This one can tile space!],
       }) ::

     ("rcocta", {(Scorecard)
       about = [The [b[rhombicuboctahedron]] can be made by exploding
                a cube and connecting the faces, or exploding an octahedron
                and connecting the faces.],
       shape-tier = TIER-B,
       shape-tier-notes = [A pleasant meeting of squares and triangles,
                           but not particularly inspired.],
       }) ::

     ("tcocta", {(Scorecard)
       about = [Kepler named the [b[truncated cuboctahedron]], but
                it’s not a proper truncation (Kepler was notoriously
                imprecise). After truncating the
                cuboctahedron you would need to fiddle with the
                resulting rectangles to turn them into squares.],
       shape-tier = TIER-C,
        }) ::

     ("scube", {(Scorecard)
       about = [The [b[snub cube]] is an inspired specimen formed
                from twisting the faces of an exploded cube just
                right so that everything can be fixed up with equilateral
                triangles. The choice of twist direction yields
                two chiral “enantiomorphs”. Calling this operation
                a “snub” does not seem fair to it, although everyone
                agrees that it makes the polyhedron cuter.],
       shape-tier = TIER-S,
       factoid = [Smallest known (to me) polyhedron that may be
                  a counterexample to the Rupert conjecture.],
        }) ::

     ("idode", {(Scorecard)
       about = [The [b[icosidodecahedron]] is kind of an icosahedron
                and a dodecahedron at the same time. It has 12 pentagons,
                like the dodecahedron, and 20 triangles, like the
                icosahedron.],
       shape-tier = TIER-A,
       shape-tier-notes = [Solid. This one definitely seems like it
                           should exist.]
        }) ::

     ("tdode", {(Scorecard)
       about = [You can make the [b[truncated dodecahedron]] by
                a shaving down a nice dodecahedron's corners into
                triangles, wrecking it.],
       shape-tier = TIER-D,
       shape-tier-notes = [A worse version of the dodecahedron.],
       factoid = [The edge lengths are all the same here, but since the
                  decagons are massively larger than the triangles,
                  there's a pretty convincing optical illusion where
                  the triangle's edges look shorter.],
        }) ::

     ("ticos", {(Scorecard)
       about = [The [b[truncated icosahedron]] is an idealized soccer
                ball, which you can get by slicing off the points of
                an icosahedron or straight from the official FIFA store.
                It’s made of hexagons and smaller pentagons.],
       shape-tier = TIER-B,
       factoid = [Albrecht Dürer tried to write down all the Archimedean
                  solids but he forgot this one![cite durer1525underweysung]]
        }) ::


     ("ridode", {(Scorecard)
                 (* Rhombicosidodecahedron *)
}) ::
     ("tidode", {(Scorecard)
       about = [The [b[truncated icosidodecahedron]] appears when you cut
                off the vertices of an icosidodecahedron, getting
                squares.],
       shape-tier = TIER-C,
       shape-tier-notes = [Flat and round at the same time. No thank you.],
       }) ::

     ("sdode", {(Scorecard)
       about = [The [b[snub dodecahedron]] can be found by exploding a
                dodecahedron, and twisting each of its faces a little
                bit so that it can be completed with strips of
                equilateral triangles.],
       shape-tier = TIER-B,
       shape-tier-notes = [Constantly in motion. But it's a bit much.],
     }) ::

     ("ktetra", {(Scorecard)
       about = [The [b[triakis tetrahedron]] is a tetrahedron where each
                face is augmented by a shallow tetrahedron, such that
                all the resulting triangles are the same.],
       rupert-tier = TIER-S,
       rupert-tier-notes = [Incredible how close this comes to not
                            making it!],
     }) ::

     ("rdode", {(Scorecard) }) ::
     ("kocta", {(Scorecard) }) ::
     ("thexa", {(Scorecard) }) ::
     ("ditet", {(Scorecard) }) ::
     ("ddode", {(Scorecard) }) ::
     ("dhexe", {(Scorecard) }) ::

     ("pitet", {(Scorecard)
       about = [The [b[pentagonal icositetrahedron]] is
                XXX
                Chiral, like its dual, the snub cube.],
       shape-tier = TIER-B,
       shape-tier-notes = [The faces look a little bit like someone
                           was trying to draw a pentagon but started
                           drawing a hexagon by accident. It is
                           admirable how they all fit together, but
                           the whole affair is a little bit
                           unsettling.],
       factoid = [This one is Rupert and it is quite easy to find a
                  witness to this. This makes it very puzzling that
                  its dual, the snub cube, does [it[not]] seem to
                  be solvable.],
                }) ::

     ("rtriac", {(Scorecard)
       about = [The [b[rhombic triacontahedron]] ought to be better
                known as the D30, a completely satisfying 30-sided die.
                It even has faces whose aspect ratio accommodates
                two-digit numbers.],
       shape-tier = TIER-A,
       }) ::

     ("kicos", {(Scorecard)

       shape-tier-notes = [Holy Christmas tree topper, Batman!],
     }) ::

     ("pdode", {(Scorecard)
       about = [The [b[pentakis dodecahedron]] is the dual of a soccer
                ball.],
       shape-tier = TIER-C,
       shape-tier-notes = [Looks great at first, but then you realize
                           that those triangles are not equilateral.],
     }) ::

     ("dtriac", {(Scorecard)
       about = [The [b[disdyakis triacontahedron]] is also known
                as the D120. Only extremely advanced Dungeons and
                Dragons players need to roll with such precision.],
       (* Looks like someone tried too hard to make a
          geodesic dome. *)
       shape-tier = TIER-C,
       factoid = [This one wins the contest for having the most
                  faces of any Platonic, Archimedean, or Catalan
                  solid!]
     }) ::

     ("phexe", {(Scorecard)
       factoid = [This is one of the rare polyhedra that is chiral.
                  We can just pick one of the forms for the Rupert
                  problem, as a solution to one yields a solution
                  for the other by just mirroring.]}) ::
     nil)

in

(main-text (body-font
[
 [title (layout doc-title)][vspace (point 5.0)]
 [sub-title (layout doc-subtitle)]

 [author[Dr. Tom Murphy VII, Ph.D.]]
 [date[March 2025]]

 [*
 1.333333333333 is [render-float 1.33333333333]

 0.000123456789 is [render-float 0.000123456789]

 123456789 is [render-float 123456789.0]

 1234567890000 is [render-float 1234567890000.0]

 0.00000000123456 is [render-float 0.00000000123456]

  [box {(Box) width = inch 3.0, height = inch 1.0 } [box one]]
  [box {(Box) width = inch 3.0, height = inch 1.0 } [box two]]

  *]

 How big is the unit cube? “1?” Seems obvious, right?
 Let me ask you another way: [it[Where]] is the unit cube? “0?”

 Or: How big is the unit sphere? “1?” Imagine I am asking you these
 questions where I pounce upon you with the next question just as
 you start to answer the first. To keep you off your balance,
 I mean. [it[Where]] is the unit sphere? “0?” Which is bigger, the
 unit cube or the unit sphere? Sweating yet?

 “Officially” speaking, the unit cube has edge length 1, and
 has all non-negative home coordinates, like ({0, 1}, {0, 1}, {0, 1}).
 According to those same math referees, the unit sphere has [it[radius]]
 1, and its center at (0, 0, 0).

 So the unit cube fits easily within the unit sphere. I don't know
 about you, but I always imagine the idealized cube and sphere
 centered on the origin, with the sphere tucked inside the cube,
 touching its sides at their centers.[footnote[The long-running
 ThursDz's Beer Society of Math Geniuses decided that what I am
 actually imagining is not the unit cube but the “L-infinity unit
 sphere,” which I think may be true but sounds like I'm just trying to
 be an asshole.]]

 ... shrinkwrap ...

 Does the unit sphere fit inside the

 Now that I’ve gotten the “dice in sphere” pun out of the way, we can
 move onto the real topic of this paper.

 Late one night I was admiring the Wikipedia article on the
 Dodecahedron, my favorite Platonic solid. On this page I was reminded
 that the Dodecahedron is Rupert, like the cube. This seemed right,
 since if the cube has a pleasing property and the Dodecahedron is
 awesome, it should also have that pleasing property. This is of
 course not true. For example, the cube can tile space and the
 dodecahedron obviously cannot. The property of having pleasing
 properties is itself pleasing, though, and throughout history
 people have ... (Bertrand Russell, Godel)

 Indeed, all the Platonic solids have the Rupert property. The Platonic
 solids are beautiful and so the fact that all of them have this pleasing
 property recommends it further. Then I read the phrase “it has been
 conjectured that all 3-dimensional convex polyhedra have this
 property,” which made my brain feel surprised but happy. I might have
 even gotten to sleep, had I stopped reading at that moment. But then I
 read “of the 13 Archimedean solids, it is known that at least ten have
 the Rupert property,” and this made my brain surprised and angry. How
 could it be the case that we think this is true for [it[all]] convex
 polyhedra (infinitely many, and mostly gigantic weird ugly ones) but
 we don’t know for some 3 simple beautiful ones? Did nobody check? It
 seemed to me it would be pretty easy to write a search procedure that
 would look for them, and it also seemed like if we think it’s
 possible, it would be easy to find solutions.

Here were my Naive Heuristics:
[ul

 ([This is a continuous problem. If you have some way of fitting the
  shape through itself, then there will be some adjacent small
  variation on that that will also work. Problems where the solution
  needs to be exact (e.g. problems on integers) tend to be much
  harder. These solutions won’t need to be exact because of [it[NO
  TOUCHING!]] ] ::

 [This might be a problem that not that many people have tried (only
  stamp collectors), since serious mathematicians would be interested
  in a real solution (i.e. a proof that the general conjecture is
  true).] ::

 [I am definitely not the best mathematician to have tried this, but
  it’s possible that I’m the best programmer to try it, and plausible
  that nobody has tried it with a very fast and hot GPU.] ::

 [I can whip this up in a day (or maybe a weekend) and then put it
  aside if I can’t solve it.] ::

 nil)]

So I set aside two and a half months to work on it.

[* XXX somewhere up here, need to introduce platonic, archimedean,
catalan, and the "wishlist" *]

This problem is pretty easy to specify precisely. The shape in
question is a convex polyhedron, which is just defined by its set of
vertices. For general polyhedra you also need to specify how those
vertices are connected (the edges and faces), but convex polyhedra are
easier. There’s just one way to stretch a “skin” over the points, so
we don’t even need to describe it (or even think about it). We’ll take
two copies of the points. One is the “outer” polyhedron and one is the
“inner”. The goal is to find some way of arranging them so that the
inner can pass through the outer.

The inner one will pass through the outer along some line, so we say
without loss of generality that this is the z axis. We’ll use the
computer graphics convention that the camera is located at some
positive z, looking down at the shapes, which are near the origin, and
the inner polyhedron is moving along this same line of sight. Maybe
like it’s shooting out of our eyeballs as a kind of abstract weapon of
geometry. A Platomic Bomb. Viewed this way, what it means for the
inner shape to be able to pass through the outer is that the two
dimensional “shadow” of the inner shape is entirely contained within
the shadow of the outer shape.

We’ll specify the arrangement of the polyhedra as a rotation and
translation; together these are a rigid frame (hereafter just
“frame”). Because we know we’re traveling along the z axis, the z
component of the translations are unimportant and we can just consider
2D translations. Moreover, since we just care about the relative
positions of the objects, we can say that the outer polyhedron is
fixed at (0,0). We need to be able to rotate both shapes arbitrarily,
though.

The inner shadow being completely contained within the outer shadow is
intuitive, but we should be more precise. The convex hull of a set of
2D points is the minimal convex polygon that contains them all (here
“contains” includes the boundary); this is the same idea as the
minimal skin around the vertices of our convex polyhedron. To get the
shape of the shadow, we just project the object to 2D along the z axis
(easy: (x, y, z) just becomes (x, y)) and then compute the convex hull
of the points using standard algorithms. Now we can just ask whether
the inner hull is entirely contained within the outer hull. Since
the outer hull is convex, this amounts to a standard test that
each point on the inner hull is contained within a convex 2D polygon.
You can find slightly buggy code for this all over the internet.
(There are many alternative formulations, some of which are discussed
below.)

The boundary condition here is very important. The inner points must
be [it[strictly]] contained within the outer hull (less-than, not
less-than-or-equal), never exactly on the boundary or coincident with
an outer vertex. If we allow them to be on the hull, then carving the
inner through the outer would make the residue disconnected (perhaps
dramatically so). It also makes the problem trivial: If the outer and
inner have the same frame, then their shadows are also the same, and
the inner one is trivially (weakly) inside the outer. If you think
this amounts to “passing one cube through the other and leaving a
proper hole”, then you and I disagree about what proper hole means.

So to solve the Rupert problem for some shape, you need to find two
rigid frames that satisfy the above (and we know that one of the
translations can be (0, 0, 0) and the other (x, y, 0)). How do we find
such frames?

If you have a fast enough test, sampling will suffice for easy objects
like the cube. Here you just generate random frames and test whether
the condition holds. You can try all orientations and reasonable
bounds on the translation (you do not want to translate more than the
diameter of the cube, for example, or it will definitely not go
through it)!

[subsection[Generating random orientations]]

Generating random numbers is easy using floating point roundoff
error.[cite murphy2023grad] How do you generate a random rotation
(orientation)? There are a few different ways to specify a rotation.
You can use Euler angles, which are three parameters that give the
rotation around the x, y, and z axes (“pitch,” “roll,” and “yaw”; see
Figure [ROLL-PITCH-YAW]). This approach actually sucks (famously,
Euler was not that good at math). You can get all orientations this
way, but you will get some orientations more often than
others.[footnote[Euler rotation is accomplished by rotating along each
axis in turn. Imagine gimbal lock: Rotate the Earth such that ... The
density of parameters that are close to this gimbal lock position is
much higher.]] Maybe that is okay for you (or Eu-ler) but I want all
orientations to be equally likely.

[image { img = image-load "roll_pitch_yaw_mnemonic.png",
         width = inch 3.65 }]
Figure [ROLL-PITCH-YAW].
Wikipedia[cite wikipedia-roll-pitch-yaw] provides this useful mnemonic
for remembering which axis corresponds to each of the three words. The
[b[pitch]]er makes sense, since you famously use a pitcher by holding
the handle away from you and turning your wrist to pour diagonally
towards yourself. But d[b[oor]] must just be trolling, right?

A good way to do this is using Quaternions, the even more mysterious
second cousins of the complex numbers. I will not try to give you an
intuition for quaternions (since I do not really have one) but they
can be used as a four-parameter representation of orientations that
will leave you happy (and puzzled) instead of sad (and puzzled). Facts
to know about the Quaternions:

[ul([Most people don’t capitalize Quaternions.] ::

    [Like complex numbers where you have a + b[b[i]], here we have a +
    b[b[i]] + c[b[j]] + d[b[k]]. The parameters are (a, b, c, d) and
    [b[i]], [b[j]], [b[k]] are “even more imaginary” “constants” that
    have some impossible relations, like [b[i]][sup[2]] = -1 but also
    [b[ijk]] = -1.] ::

    [You can just think of a quaternion as a four-dimensional vector
    (a, b, c, d). If this is a unit-length vector, then it represents an
    orientation. There are exactly two unit quaternions representing
    each unique orientation in 3D. No gimbal lock and no favorites.] ::

    nil)]

A good—but not great—way to generate random 4D unit vectors is to
generate random points on a 4D hypersphere, because these are the same
thing. There are very fancy ways to do this, but you run the risk of
getting the math wrong, or head explosion etc., so I recommend
rejection sampling. Rejection sampling is a very robust way to
generate uniform samples in some set. What you do is generate random
points inside some domain that contains the target set, and throw away
points that aren’t in the target. For example, to generate points in a
unit circle, you can generate points in the 2x2 square (it’s [it[not]]
the unit square) that contains that circle. π/4 of these points will
be in the circle, and so you get samples at an efficiency of about
78.5%.

[image { img = image-load "sample-circle.png", width = inch 3.65 }]

To generate points inside a sphere, you do the same thing, but in a
2x2x2 cube. This sphere has volume 4π/3 and the cube has volume 8,
so you get samples at an efficiency of about 52.4%.

To generate 4D points inside a 4D hypersphere[footnote
[We should say “3-sphere,” or “hyperball”, since the convention
 is that a normal sphere in 3D is called a 2-sphere, since its
 surface is actually two dimensional. It would just seem to add
confusion here, though.]], you do the same thing,
but now the hypervolume is π[sup[2]]/2, and the 4D hypercube has hypervolume
16, so you get samples at an efficiency of about 30.8%.

Upsettingly, as we increase dimensions, the hypervolume of the
[it[n]]-dimensional hypersphere approaches zero (!?) and the
[it[n]]-dimensional hypercube's grows exponentially, so this
technique approaches perfect 0% efficiency.

[image { img = image-load "sample-nd-hypersphere.png", width = inch 3.65 }]

Fortunately, we only need 4D vectors, and 30% efficiency is fine
because my computer can calculate like 1 billion samples per second
and I only need two.

The two samples give the orientations of the outer and inner polyhedra,
and we also pick random positions. We then project to 2D, compute
the convex hulls, and see if the inner hull is inside the outer hull.

[subsection[The convex hulls]]

The 3D shapes are convex, and so their 2D shadows are
convex.[footnote[It is not completely obvious that this must be true.
One of the ways to believe it follows from a definition of convexity:
For every pair of points in the set, the entire line segment between
them is in the set. To show that the 2D shadow is convex, take any two
points in it. These points correspond to some two points in the 3D
shape, which means (by that definition convexity) that the line
segment between them is in the 3D set. The projection from the 3D
shape to the 2D shadow also transforms that line segment to a line
segment (the projection is [it[linear]]) and it connects the 2D
points. So this satisfies the definition of convexity for the 2D
shadow. In fact, [it[all]] linear transformations preserve convexity
by the same argument.]] Rather than just working with the set of
points, their boundary polygon is a much more convenient
representation of the shadow. Here is a shadow of the icosahedron. The
darker boundary polygon is its 2D convex hull:

[image { img = image-load "icos-shadow.png", width = inch 3.65 }]

Computing the convex hull is also “standard,” meaning that you can
find lots of slightly buggy implementations of various algorithms on the
internet. The bugs are usually because the routines are intended for
computer graphics and so they don't have to “work,” and because the
algorithms are conceptualized in the mathematical world where when you
look at a point that's really close to a line segment, the point stays
on the same side of the line when you look at it from different
directions. This is unfortunately not the case for naive
implementations using floating point. It usually “doesn’t matter that
much,” or “just add a magic constant you named epsilon,[cite
murphy2014epsilon]” but unfortunately when you are working with
extremely regular shapes like Platonic solids, you will frequently get
points that are colinear or coplanar and exercise the too-optimistic
beliefs of the code you found. So this is another good way to make
your afternoon project take several months.

You only need to compute the outer hull; you can then just check that
all of the inner shadow's [it[vertices]] are inside it. But I found it
was faster to compute a convex hull for the inner polyhedron as well.
That way you only need to do the point-in-polygon test for the points
[it[on]] the inner hull. [* XXX incircle? *]
The point-in-polygon test is standard; we just have to make sure we
are testing that the points are [it[strictly]] inside, and not on
the hull itself.

[subsection[Optimizing]]

Now we can test whether some random orientations and positions (frames)
demonstrate the Rupert property. It is easy to find solutions for
the cube by just sampling. But of course we want to make it faster,
first of all just for the heck of it, but also so that we can solve
the unknown cases, which are presumably harder.

I started with black-box optimization, again using my own twisted
variant of BiteOpt.[cite vaneev2024biteopt] Black box optimization
is good for people like me and Euler who are bad at math. The interface
to such an optimizer is a function like

[monospace-block[double F(double a1, double a2, ..., double an)]]

For some fixed n. The optimizer doesn't know what the parameters
mean; its job is just to find the arguments ([tt[a1]], ..., [tt[an]])
such that [tt[F(a1, ..., an)]] has the smallest value. This is
of course impossible in general,[footnote[Not just hard because
the function could be complicated. It's literally impossible due to
diagonalization. Take for example the recursive function
[tt[double F(double x) { return -abs(x - Optimize(F)); }]]. This computes
its own minimum, and then returns the negated distance from the argument
to that supposed minimum. This makes the purported minimum actually the
maximum (0) with a nice convex triangle all around it. In reality
this function will just loop forever, since Optimize works by
calling the function many times. [* Also, in reality, does this function
have a “minimum” at infinities or nan? *]
]]
but for many well-behaved functions these optimizers are nonetheless
able to do a good job.

Here the arguments will be the orientations of the two polyhedra and
the position of the inner one. We can represent the orientations
with quaternions (four parameters each) and the position as the (x, y)
offset, totaling ten parameters.

The optimizer does need some kind of surface to optimize over; it does
not work well if there is just a single point where the function
returns -1 and it is a flat 0 everywhere else. I tried several
approaches here. The one that worked best for me was to take all the
vertices on the inner hull that are [it[not inside]] the outer hull,
and sum their distance to the outer hull. This prefers the vertices to
be inside where we want them, and increases the penalty as they get
further outside. It is essential to add a nonzero error when the
point is not strictly inside; if the point is exactly on the hull or
the distance rounds to zero, we still need to add a small positive
value. Otherwise the optimizer will quickly find degenerate
“solutions” such as setting both orientations the same.

The other rub is that the optimizer wants to try any value (within
specified bounds) for the arguments, but we need each orientation to
be a proper unit quaternion. Simply normalizing the four inputs would
work, but as we observed before, random samples in this
parameterization are not uniformly random orientations. My approach
here is to first choose actually random quaternions for the outer and
inner shape before beginning optimization. I then optimize within
fairly narrow bounds (like [layout "-.15, +.15"]) for the quaternion
parameters, and add that as a “tweak” to the random initial
quaternion, normalizing to get a proper orientation. This is still not
uniform, but it is locally closer to uniform. Since we will try
optimization millions or billions of times from uniformly random
starting orientations, we will get good coverage of all orientations.
Other parameterizations of the orientation are possible. It is
definitely desirable to have fewer arguments (as the complexity
naively grows [it[exponentially]] in the number of optimization
parameters), but simply using the three-parameter Euler angles runs
into the aforementioned problems.

[subsection[It works!]]

Anyway, that works! This was like, the first weekend of the project.
It's able to find solutions to the cube in milliseconds, and so I
added more polyhedra to the collection, and solved those in
milliseconds as well.

One of the most tedious parts of this was getting all of the polyhedra
represented in computer form. I was somewhat surprised that the
formulas for these things often involve wacky irrational coordinates
like the “tribonacci” constant, which is like the Fibonacci (Fi- means
two, like in the number Five) but where we take the sum of the previous
three numbers instead of two. The ratio of terms converges to:

(1 + [cbrt](19 + 3√33)) + [cbrt](19 - 3√33)) / 3

Like, I would expect √2 stuff. But I guess I should not have been
surprised by that, because that's just math. Anyway, since these
are all convex polyhedra, at least you don't need the

... xxx solves them all, but not the unknown ones ...


[subsection[Alternate solvers]]

Of course we should check uniformly random configurations, but
I tried some other approaches as well:

[paragraph[Max]] This first optimizes the outer shadow so that
it maximizes its area. We then perform optimization only on the
inner shadow. Intuitively, you want the outer shadow to be “bigger”
and the inner shadow “smaller,” so this makes sense as a heuristic
and reduces the number of parameters. Largest area does not
mean it is best at fitting a given inner shape, though.
This strategy can solve all the polyhedra (with known solutions) except:
[layout (polyhedra-without-method METHOD_MAX)].

[paragraph[Parallel]] Thinking about making the inner shadow
as [it[small]] as possible, we see that we often (always?) reach
a numeric minimum when at least one face is parallel to the
projection axis; this face then becomes zero area in the shadow.
This strategy chooses two non-parallel faces of the inner polyhedron
at random, and then orients the polyhedron such that these are both
parallel to the z axis. It also rotates the polyhedron around the z
axis such that one of these faces is aligned with the y axis (this
doesn't really change anything except to make the numbers rounder
and the hulls easier to interpret, e.g. the cube will always be an
axis-aligned square). Then we just optimize the outer orientation
and position to fit around this hull.
This strategy can solve all the polyhedra except:
[layout (polyhedra-without-method METHOD_PARALLEL)].

XXX image may be helpful here

[paragraph[Origin]] Optimize both rotations, but leave both polyhedra
centered on the origin. This reduces the number of parameters,
although the translation parameters are the best behaved of the bunch
(optimizing the translation parameters alone is actually a convex
problem). The main reason to do this is to see whether there are
always solutions that have this form. It does not appear to be the
case: The tetrahedron-like shapes seem to [it[require]] translation. I
did not prove this, but given how narrow the clearance is for the
triakis tetrahedron (or how simple the tetrahedron is), it may not be
hard to do so. [* XXX Z3? *]
This strategy can solve all the polyhedra except:
[layout (polyhedra-without-method METHOD_ORIGIN)].


[paragraph[Special]] Combines parallel and origin, leaving only the
other rotation to optimize. Like the origin approach, the main reason
is to see whether solutions of this form exist; it turns out to work
in all the same cases as the origin method. This is all of the
polyhedra except:
[layout (polyhedra-without-method METHOD_SPECIAL)].


[subsection[GPU solver]]

At this point, I was easily solving the polyhedra with known
solutions, like each in a few hundred milliseconds, and not at all
solving the other ones. I figured one possibility was that these were
just harder, and so I needed to be able to optimize the solver to try
a lot more times. One way to try a lot more times is to do it on a
Geometric Polyhedron Unit. Part of the way I justify to myself buying
the world's physically largest[footnote[It's comically large. I
literally broke my computer trying to install it, and had
to buy an entirely new computer with a bigger case just to fit
it in there.]] and hottest GPU (at the time), the NVidia RTX 4090,
is that I can use it for important tasks like this and not just
sniping simulated soldiers in glorious 4k HDR at 144fps. So I rewrote the
solver in OpenCL.

In some ways this problem is well suited to the GPU; it excels at
parallel numerical tasks on floating-point numbers. The polyhedra here
are too small to benefit from parallel computation on their vertices.
But we can easily get massive data parallelism by trying multiple
optimization instances in parallel. On the other hand, the convex hull
calculation and black box optimizers are not natural for the GPU
(OpenCL does not really support recursion!).

To test whether the inner shadow is within the outer shadow, I
replaced the convex hull-based test with one that is worse but more
easily parallelized. For each polyhedra I generate its triangulation,
where each face is made with triangles (this is trivial to do with
triangle fans because they are convex polygons). Now observe that when
I project these triangular faces to the 2D shadow, any point that is
contained in the shadow will be contained in at least one of these
projected triangles. I can check all of the triangles in parallel. I
can also compute the error for a point as its shortest distance to any
triangle (like we previously used the shortest distance to the hull).
The point-in-triangle tests must be [it[strict]] as before, to
prevent points exactly on the outer boundary from counting. Alas,
this test is not quite correct here: It is possible for an interior
point to land [it[exactly]] and [it[only]] on interior edges of
the triangulation. Take an axis-aligned cube, for example; the
point at the exact center of its square shadow will lie on edges
of the triangulation, no matter which one you use. This is not
ideal, but it only gives us false negatives (failing to find a solution
if one exists), which is not a serious problem.

[* XXX good place for diagram *]

Because I did not want to port the black-box optimizer, and because we
can do better anyway since we understand the problem being optimized,
I implemented a proper gradient descent optimizer for the GPU. This
subject is well documented so I will not belabor it here, but I
performed “approximate numerical differentiation” to compute the
derivate with respect to each parameter independently. This involves
evaluating the function one additional time for each parameter (with a
small tweak), assuming that the slope is locally linear. It's not too
bad to implement, but since this problem has 10 optimization
parameters, it is a significant amount of additional evaluation. I
don't think this problem lends itself well to analytical derivatives
(even though most of the space is very smooth, the regions of interest
are near the boundaries, either as a point moves into its own shape's
shadow, or across the other's hull), but maybe you or someone else
who's smarter than me could figure it out. Lazy people would use
automatic differentiation and might be happy with that.

Anyway, this all works too! It is indeed faster than the CPU version
(about XXX times faster), although it is harder to play around with
algorithmic tweaks and it scales worse to polyhedra with larger
triangulations. I mainly found solutions using the CPU methods, and
mainly because running things on the GPU means I can't simultaneously
use my computer for other important activies like over-the-top violent
first-person shooter games.

[subsection[Solved!]]

And then I found a solution for one of the wishlist polyhedra!
Actually all of them. I didn't get too excited, though; there had
been many false positives so far (due to bugs), and the reported
numbers were like this

[monospace-block[
outer frame:
-0.99999999999999978,-3.7558689392125502e-16,-3.5847581116984005e-08,
3.7558689392125502e-16,1.0000000000000002,-2.0954657592967021e-08,
3.5847581116984005e-08,-2.0954657592967021e-08,-0.99999999999999956
0, 0, 0

inner frame:
3.3306690738754691e-16,0.99999999999999978,5.551115123125779e-17,
2.7755575615628914e-16,-5.5511151231257852e-17,0.99999999999999978,
0.99999999999999978,-2.2204460492503128e-16,-2.7755575615628909e-16,
-1.4197330001097729e-18,2.8394660002195473e-19,-0.0051151272082079749

Ratio: 0.9999999999999999766]]

Note how everything is either really close to 1 or zero.
Recall that two equal frames produce identical shadows, and
that these are invalid Rupert configurations (the “hole” eats
the entire shape). So too when the orientations are the same
up to symmetry (e.g. one rotates the cube 1º and the other
91º). So I knew it was possible that we could get something
really close to identical shadows, but that they might look
like they satisfy the condition within the precision of
double-precision floating point numbers. Also, given my
fetish for IEEE-754, I'm certainly asking for it! Visually
inspecting these solutions, this is exactly what they looked
like.

[b[ON THE OTHER HAND]], some solutions can have a lot of
nines in them! For example, the best known solution for the
triakis tetrahedron comes within [it[one one-millionth]] of the
radius of the polyhedron, requiring a monumental amount
of zooming-in to even perceive this thread as having volume.
This would be a good reason that nobody found these solutions
before: Perhaps they used single-precision floating point,
or coarse values of “epsilon,” or rejected them with visual
inspection?

So I invested further effort.

[subsection[Rational solvers]]

The right way to deal with floating point inaccuracy is to not use
them. Lots of geometry will work great with other number systems, so
with a little finesse we can work on this problem using rational
numbers and sidestep the numerical problems. There are just
a few problems:

[paragraph[Shapes are not rational]] Most of the polyhedra considered
do not have vertices with rational coordinates! The cube is easy, but
even something as canonical as the dodecahedron has some points on
integer coordinates and others on φ coordinates, and there's no way to
scale the shape so that everything is rational. To solve this, I
implemented rational approximations for each of the shapes, where you
can decide ahead of time on an arbitrarily small epsilon
(alternatively, a number of digits of precision) for the coordinates.
For these shapes you just need a routine that can compute square and
cube roots to arbitrary precision.[footnote[One non-obvious but
important thing to consider here: For a given accuracy goal, there are
infinitely many rationals that fall within that range. Most of these
are bad choices because the numerator and denominator are enormous
numbers. So it is important that we not just generate a rational that
is close to the target value, but that we generate a reasonably
compact rational; otherwise downstream computations need to do a lot
more work. Many classic approximation algorithms do not account for
this, since for example the efficiency of a float does not depend much
on its specific value.]] (I also did π, which is fun, before
realizing I don't even need it.) Since the resulting shapes are not
exact, any solution we find might only work for the slightly
inaccurate shape, but once we have a solution we can verify it by
other means. It's also possible we would fail to find a solution
(because the required precision is still too low), but then we can
try again with higher precision.

[paragraph[Search procedure needs roots]] The search procedure we
have been using so far involves a few operations that are not
available for the rationals. For example, our error function involves
the distance between a point and the hull, which needs a square
root (square roots of rationals are not necessarily rational).
This is easily handled by just using the squared distance as the
loss function (this is common even with floats and sometimes
works better!) A little trickier is rotation. Before we used
[it[unit]] quaternions to represent orientations. Normalizing
a quaternion means dividing by its length, which involves a root;
we can't do this with rationals. Fortunately, we do not actually
need unit quaternions. The rotation induced by an arbitrary q (other
than the zero quaternion) can be given as

  rot(v) = q × v × q[sup[-1]]

and if you work this all out, you find that the quaternion's
length is only used [it[squared]], which means that you never
need to calculate the root. This means that if we start with
rational coordinates, we can represent orientations as non-unit
quaternions, and get rational rotated coordinates. Rational
translation is trivial. All we have to do is make pure rational
versions of the convex hull calculations (mostly just needs
cross product; these become much cleaner when you know you have
exact line-side tests due to exact representations of the
points, too) and point-in-polygon tests, and so on. Rational
arithmetic is like millions of times slower than floating point,
but other than that, it's really nice!

[paragraph[The optimizer is still double-based]] Now we can represent
arbitrarily fine rotations and translations exactly, but the optimizer
is still working on double-precision numbers. This is easily handled
by scaling down the parameters before running the error calculation.
For example, if the optimizer asks to try a value of 0.123 for a
parameter, we convert that to a rational, and then divide it by
2[sup[20]] or something large so that we only work in a very narrow
range around the initial value. This scale is chosen randomly and
independently for each optimization parameter.

I got this working. We are primarily interested in seeing if there are
actual solutions near the supposed ones that may just be floating
point error. I use those solutions as starting points for
optimization, as well as two random equal rotations and no
translation. Alas, the purported solutions are not actually valid,
and they do not seem to be close to any solutions. I ran the rational
search for many days on the unsolved polyhedra with no joy.

[section[Noperts]]

By this point I was feeling pretty confident that the Rupert conjecture
is actually false. This would certainly explain why nobody had
solved these five polyhedra before! And so I set out to try to find
more (conjectured) counterexamples, in the hopes of gaining some
insight or at least advancing the state of the art in some small way.



XXX noperts
XXX symmetry

[subsubsection[Epsilon]]

Speaking of epsilon, and my obsession with minutiae related to it,
it itself a kind of minutiae: Most numerical code (including this
Rupert solver) has a line like this in it:

[monospace-block[return std::abs(x) < 1.0e-6;]]

Here [tt[1.0e-6]] is one one-millionth, a typical value for epsilon.
It's actually a pretty nasty choice since it is not even representable
as a float. With clang 19, this assembles to code like

[monospace-block[.LCPI0_0:
        .quad   0x7fffffffffffffff
        .quad   0x7fffffffffffffff
.LCPI0_1:
        .quad   0x3eb0c6f7a0b5ed8d
Threshold(double):
        andpd   xmm0, xmmword ptr [layout "[rip + .LCPI0_0]"]
        movsd   xmm1, qword ptr [layout "[rip + .LCPI0_1]"]
        ucomisd xmm1, xmm0
        seta    al
        ret]]

which makes sense ([tt[ucomisd]] is unsigned comparison of
floating-point registers) other than the two copies of [tt[0x7fff...]]
(?). But another thing I tried was to optimize this epsilon test,
instead writing the clear and portable

[monospace-block[static constexpr uint32_t target_exp =
  (std::bit_cast<uint64_t>(0x1.0p-20) >> 52) & 0x7FF;

uint32_t exp =
  (std::bit_cast<uint64_t>(d) >> 52) & 0x7FF;
return exp < target_exp;]]

This checks against a different epsilon (the power of two close
to one one-millionth) by just checking the exponent bits directly.
This compiles to the much more pleasant

[monospace-block[        movq    rax, xmm0
        shr     rax, 52
        and     eax, 2047
        cmp     eax, 1003
        setb    al
        ret]]

It is not clear that this code is actually faster; it probably saves a
few cycles of latency but vectorizes worse. It was a total wash in
benchmarks. However, I spent some time arguing with AI about it, and
eventually won. Like a coward, it weasled out of a formal apology:

[image { img = image-load "i-win-again.png", width = inch 3.65 }]

[* In general, I spent a lot of time optimizing... *]

[section[Escape COD]]

Another GPU-based method I tried was to 100% the multiplayer mode of
[it[Call Of Duty: Black Ops 6]]. It's not the sixth [it[Call of Duty]]
game (come now), it's the sixth [it[Black Ops]] game!

To me, “100%” meant:
[ul ([Get to Prestige Master] ::
     [Get the “Multiplayer 100%” badge] ::
     [Get “mastery” for every item in the game.] ::
     nil)]

[paragraph[Prestige Master]] It is easy enough to max out your level
to 55 (?) in this game, but then you can “Prestige” (jargon verb
meaning roughly “shame”) and reset your progress, allowing you to make
meta-“progress” through ten levels of Prestige, and then 1000 levels
of “Prestige Master.” This allows you to feel neurotransmitters when
you “unlock” something for the second, or third, or tenth time. The
neurotransmitters are necessary due to the receptor desensitization
caused by the constant stream of messages and medals telling you how
good you are, or how many points you got, or how hard you killed six
or seven guys at the same time by spamming them with grenades.[cite schulz1998predictive]
This is the easiest thing to do, since it just happens by getting
points from playing the game, no matter how you do it.

[paragraph[Multiplayer 100%]] This is essentially an achievement list.
Most of them happen naturally by just playing, but some require an
irritatingly specific set of circumstances (“With the enforcer Perk
Specialty active: get 10 kills while War Cry is active in a single
match”) and so they require playing a lot, and in a specific way.
For calibration, simply completing this list is apparently enough
content for 365k views in the genre of “I played video games a lot”
on YouTube.[cite roberts2024unlocked]

[paragraph[Every item mastery]] This is the most tedious. Mastery
means you did the thing a lot. You get mastery for a weapon for
getting 500 kills with that weapon, for example. For good weapons,
this is easy and actually fun. For the many bad weapons, it is an
awful grind. For example, there are these rocket launchers that are
mainly designed for shooting down helicopters, but if you can manage
to fire them at a human without getting killed before you finish
looking down the sight, and you land a basically direct hit on their
soft body, then you get a kill. Just 500 of those! Then there are
weapons seemingly designed just for humiliating your opponent, like a
hand-held power drill that you can drill into them twice at close
range. Just 500 of those! Worse is the scorestreaks, which you
activate by getting a certain number of points per life—generally a
lot—and some of them will only do their thing in certain situations
(like interceptors, which destroy airborne enemy scorestreaks).
Thankfully these only need 100 kills. Then there are field upgrades,
which are on a timer that only activates a few times per match. So
that means that you only get a few attempts per game to disorient and
then kill some enemies with the pathetic “neurogas” item, or to
perform a “tactical insertion” and then kill an enemy within five
seconds of being born. Worst of all are the “non-lethal equipment,”
which includes items seemingly designed for a different game, like the
“proximity alarm.” This thing alerts you when there is an enemy—which
there always is—and then maybe if you kill the enemy while the alarm
is beeping, it registers progress towards mastery. So after spawning,
you hope that you can quickly throw a proximity alarm on some nearby
wall and kill an enemy that you were going to kill anyway, all the
while trying to intuit the undisclosed logic by which it will count
this as a “proximity alarm assist.”

[image { img = image-load "cod-mastery.jpg", width = inch 3.65 }]

Anyway, I finished Cube Octahedron Dodecahedron: Block Ops 6 on
the evening of the SIGBOVIK deadline, 28 Mar 2025—after some 178 hours
of active in-match time—and escaped this game.

Note: I am not in any way recommending this game. I simply got
addicted to it, since sometimes I need to keep myself awake until 2am
with eyes dry from being transfixed to a flashing computer screen
while I white-knuckle the mouse and keyboard, grinding for
achievements. It is essentially artless (except sometimes by
accident), and I only played it because I am a Counter Strike Idiot.
You could perhaps use it for anthropological study if you are
interested in a disturbingly high density of people for whom their
love of Donald Trump is so important to their identity that they
cram it into their 16 character character alias.[cite healey2016proving]
The only thing I unironically like about this game is that when a
match ends, you endure a few seconds of slow-motion invincibility, where
environmental boundaries will not kill you. With good planning, this
lets you explore the outskirts of the deathmatch map beyond where you
would normally be able to reach (for example, in Stakeout, you can
jump to a nearby building and run up its stairs to a balcony).
Although you are invincible and cannot die, environmental effects like
drowning still apply. If you jump into water at this point, you will
start to suffocate through the post-game sequence, and can wind up
extremely asphyxiated at the same time you do your victory dance in
the winner's circle: True success!

Aside from the fact that this could run simultaneously with CPU-based
solvers, this approach surprisingly did not yield any results for
the Rupert problem.

[section[Other approaches]]

* Z3
* It's decidable??


[section[Miscellaneous TODO]]

Infinitely many
What’s the best Rupert?

[section[Results]]

[subsection[Scorecards]]

This section lists the results for each of the Platonic, Archimedean,
and Catalan polyhedra. If the polyhedra has a known solution, the
residue with the highest [it[clearance]] is shown. [it[Clearance]] is
defined as follows: Take the minimum Euclidean distance [it[c]]
between the 2D inner and outer hulls, and the radius [it[r]] of the
smallest sphere that contains all points in the polyhedron. Clearance
is then [it[c]]/[it[r]]; the radius is just a normalization term so
that this does not depend on the scale of the polyhedron. The
[it[ratio]] is another quality metric, which is the area of the inner
shadow divided by the area of the outer shadow. All else equal, a
lower ratio is better, but some low-ratio solutions look bad
because they have very thin walls. A third obvious choice would be to
maximize volume of the residue solid, but this is computationally
expensive and might anyway have the same thin-wall problem as ratio
does.

[layout-concat (list-map (make-scorecard scorecards) POLYHEDRA)]

[section[Improvements to BoVeX]]

To make my life harder, but also more thrilling, I typeset this
paper in BoVeX, which is a document preparation system I wrote
as a joke (?) for SIGBOVIK 2024.[cite murphy2024badness] You can
tell from the way that the math looks like a child typeset it
that I didn't yet implement any fancy layout algorithms for that.

I did, however, spend precious vacation days in the run-up to SIGBOVIK
2025 adding features and fixing other, less important deficiencies of
BoVeX so that I can continue my demented quest to use primarily
software written by myself as a joke (?) instead of the perfectly
decent mainstream software that everybody else uses, and whose lives
are therefore presumably not thrilling in this way. So begins the
Tom 7 SIGBOVIK tradition of listing [it[BoVeX improvements]]:

[paragraph[Unicode]] BoVeX now supports Unicode fonts. I needed this
so that I could write π when I was on a digression about sampling
quaternions. This was so annoying to implement! PDF was defined during
the era where we were just finally realizing that our approach to
character sets and font encoding was unsustainably complicated, and so
they tacked on Unicode as a hack on top of that complicated mess. So
you get all the benefits of the complexity of Unicode and all of the
benefits of the complexity of not Unicode. You actually have to manage
the glyphs yourself, for example, but also tell PDF how big everything
is (but also how big it [it[might be]], just in case it's inconvenient
to actually render it) and you also have to tell it how to decode the
glyphs back into Unicode so that you can search or copy-paste from the
PDF. Ugh! There are a number of undocumented or barely-documented
requirements, and the symptoms of mistakes are that Adobe Acrobat will
tell you “Unable to open [tt[test.pdf]]. Please contact the document
author.” Um, I contacted myself but nothing happened! But now you can
just put UTF-8 in your BoVeX source code and it'll work. Check this
out: Дональд Трамп может поцеловать мою задницу!

[paragraph[FixederSys]] Along those same lines, I extended the
FixederSys font family[cite fixedersys-site] with a lot more Unicode
characters, like the many exotic mathematical symbols that nobody has
ever used. Unicode is even more inspiring to notation fetishists than
[tt[amssymb]] in this regard. It's too bad that the math in this paper
is so elementary, or else we could be like [b (fixedersys[A ≽ B ⊯ ⊱∔C
⊶ ∷D⊰])].

[paragraph[“Robustness”]]
BoVeX no longer crashes programs like Adobe Acrobat that expect
the PDFs to be “correct.” LOL!!

[section[List of open problems]]

Can we disprove the universal Rupert conjecture, by proving that
one of these nice symmetric polyhedra does not have the property?

Or, can you find a solution to one of these unsolved polyhedra,
demonstrating that I am a bad programmer?

Harder: If the conjecture is false, can we show that the snub cube is
the polyhedron with the fewest vertices (24) that fails it?

Easier: Can we prove that for some polyhedra (e.g. the regular tetrahedron),
any Rupert configuration involves a translation (i.e. the projected
origins do not coincide)?

[section[Lessons]]

This is not the first time I have gotten myself sucked into a
simple-seeming open math problem and then toiled away at it for
some time, and did not solve it. I do not wish to divest myself of
the hubris instinct XXX

[section[Conclusion]]

This paper essentially does not advance the state of human knowledge
in any way.

[paragraph[Acknowledgements]] I like to think that the upsetting facts
that (a) I am well sick of this project at this point and (b) I didn't
solve it are due to an unusual (for me) approach I took with it. That
is: I talked about it openly with my friends, and even collaborated.
David Renshaw created some excellent animations that appear in the
accompanying video, and his own soothing music video. He also found
several bugs in my code, most importantly that my computed vertices
for the disdyakis triacontahedron were incorrect! Jason Reed made a
“boring, hard video game” version of the problem [* XXX link here if not
above *] you can do in your browser. Tom Lokovic, who shares my
self-defeating Gen-X distate for modernity, drew upon his 1990s
computer graphics wizardry to work through a few puzzles with me. All
of the Brain Geniuses at ThursDz's and Henge Heads Lunch [it[at a
minimum]] tolerated me repeatedly talking about polyhedra, and many
had suggestions as well. However, all of these suggestions were
ultimately fruitless or perhaps even harmful.

[section [Bibliography]]

[bibliography()]

]))


end

(*

now, most rich ..

 point set version

 small noperts

 z3 version
   .. decidable

 failing as fast as possible

triangle man hates particle man

 upsetting symmetry thing

 you get objects with names like these:
 https://mathworld.wolfram.com/Octahedron.html
 "gyrobifastigium"

 triangle man hates particle man (degenerate triangles)

 what's the best platonic solid?
 icosahedron has a triforce in it (orthogonal projection with face
 parallel to view plane)

 floating space-yogi holding shapes

 meta ruperts.
 We'll always be asking if a shape can fit inside a convex
 solid, which is the original e.g. cube.

 But the cutting shape can be concave. One example would be a
 complex of the inner/outer polyhedra in a rupert configuration.
 Note that the z coordinate of the inner one is not fixed here
 (we could consider it infinite?)

 Then the question is whether we can pass one of these complexes
 through the original solid, giving us a level-2 rupert, and so on.

 If the extrusion is infinite, then this means we won't rotate the
 inner complex, only the outer. So the question becomes: Can we
 find an initial configuration, and a series of outer rotations,
 each of which contains the previous? It's pretty clear that it
 cannot reach a fixed point, since the area of each has to be
 strictly increasing. But it is possible (likely?) that there are
 chains of infinite length (if any exist at all).

 Rather than think of the extrusion as infinite, we could put the
 inner polyhedron at a position of our choice (along the penetrating
 axis). It doesn't even need to be overlapping the outer polyhedron,
 but it seems like you want to, because the next question is whether
 you could pass this whole complex through another original
 polyhedron. Here, rigid transformations of both the outer shape
 and inner complex are permitted (and you have the additional
 optimization parameter of the inner-inner shape's z position).
 Could fixed points exist for this? Maybe?

  it's like a katamari ball

 heuristics: "inadequate equilibrium" is possibly interesting
 here, because it does have this dimension analogy (here it
 is a Blessing of Dimensionality): When optimizing two opposing
 forces on a 1D line, you have no choice but to get stuck.
 When you have two dimensions, it is possible to get trapped in
 a local maximum. Adding more dimensions makes the optimization
 problem potentially easier to solve simply by descent.

* re-search *

 communing with perverted aliens

 man-made horrors like,
   - typos in unicode
   - conventions about the size of a unit sphere and cube
   - VRML

 TODO:
  - vector/raster image wrapper, so that both are happening in tandem
  - rendering bugs look dope
  - reencode PNGs before embedding, perhaps in pdf.cc
  - some objects have inside-out normals?
  - emit floaters (footnotes)
  - math rendering!
  - video citation type

 *)
